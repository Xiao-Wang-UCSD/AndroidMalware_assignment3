# Part 1
___
## Introduction
### Background 
Android is a mobile operating system based on Linux kernel and other open source softwares. Although the marketshare is the highest, we can't deny the fact that there are many malwares that take advantage of Andriod's backdoors. 

### Objective
We are investigating Android malwares through reverse engineering, decompiling APKs and process them to smali files. The data we have are cralwed from APKpure.com, a third-party Android app market, which consists of mainly benigh apps. In my project, we will look at 100 apps across five catagories: travel, music, game, design and productivity.
___
## Data Ingestion

### APKs to Smali
In `etl.py`, you will find the first half data ingestion process. I extract the links from apkpure.com/sitemap.xml and label their catagories. Then I have a generic function that select a list of catagories you want to download. User can cap the maximum size of apps to download to avoid unusual gigantic apps that overflow User's disk. Once apps are downloaded, another bash script is set default to run and use `apktool` to convert the APKs to smali files at desired location specified in `config/data-params.json`.

### Smali to DataFrame
In `model.py`, there are parser functions that loop all the smail files and extract `invoke method`, `package name`, `API name`,`return type`, `method name`, `app`.<sup>1</sup> Thus, my data schema is a large csv file. Specifically, I have 1.75M rows in the 100 benign apps.



<span>[1]</span> <sup>I did try to parallelize this process but when I scale it up, it encounters tricky file io issues. So I improved loop efficiency by pre-compiling regex, revising code logic and removing unnecessary visits. I also capped the maximum files in each app to be 1000 because it will be more stable when encountering unusual large files.</sup>
___

## EDA and Cleaning

### EDA on Benigh Apps

| catagory     |num_of_apps| avg_num_API | avg_num_unique_API | unique_api_ratio |
|--------------|-----------|-------------|-------------|--------------------|
| travel       | 20 |16491.6     | 2498.29     | 0.151488           |
| design       | 20 |14203       | 2341.75     | 0.164877           |
| game         | 20 |18241.3     | 2444.35     | 0.134001           |
| music        | 20 |16264.5     | 2507.55     | 0.154173           |
| productivity | 20 |19588.8     | 3318        | 0.169383           |

It can be observed that productivity apps have the highest average number of APIs, unique API and unique API ratio. This could in part reflect that on average, productivity apps are more diverse, because productivity apps tend to require to integrate with others so they need more diverse APIs.

### EDA on Benigh and Malware Apps

| catagory | num_of_apps | avg_num_api | avg_num_unique_api | unique_api_ratio |
|----------|-------------|-------------|--------------------|------------------|
| benigh   | 100         | 17131.2     | 1985.26            | 0.115885         |
| malware  | 33(families)          | 27502.8     | 2086.24            | 0.0758555        |

It can be observed that malware actually has a much higher average number of APIs but a much lower number of unique APIs. I might contribute the discrepancy in average number of APIs in the data generating process because first, I only included apps that are smaller than 100MB and they tend to have fewer APIs. However, the fact that malware has a very low unique API ratio may reveal the fact that malware tend to use less diverse APIs. Although I initially suspect that this might be due to code obfuscation, after I made sure my algorithm will identity two APIs are the same only if they are the same name and same package, I think it's not due to obfuscation.

| Catagory 	|         Top 5 API         	|     Top 5 Package 	|
|----------	|:-------------------------:	|------------------:	|
| benign   	| StringBuilder append      	| StringBuilder     	|
|          	| StringBuilder init        	| android/os/Parcel 	|
|          	| StringBuilder toString    	| String            	|
|          	| android/os/Parcel recycle 	| View              	|
|          	| java/lang/String equals   	| ArrayList         	|
| malware  	|    StringBuilder append   	| StringBuilder 	|
|          	| StringBuilder toString    	| Class             	|
|          	| StringBuilder init        	| String            	|
|          	| java/lang/Class/forName   	| Throwable         	|
|          	| Throwable getCause        	| reflect/Method    	|

It can be observed that `append` is the most popular API and its package `StringBuilder` is the most popular package. In the actual data, `append` API is around 6% of all APIs and `StringBuilder` is around 10% of all packages. To decide whether to strip away `StringBuilder` or `append` can be tricky. Our kernal method is based on the relationships of API calls rather than any single APIs. On the other hand, these overly popular API/pacakges might introduce a lot of similarities among the benign and malware. In my approach, I didn't remove them because they are no more than 10% anyway. In the malware apps, the 5th popular API is getCause and 4th package is `Throwable`. This can be somewhat suspicious because `Throwable` is not even observed in the top 20 packages in benign apps. Assuming the reverse engineering process was done right, we might infer that malware apps tend to call `Throwable`, the superclass of exceptions, more often than benign apps.

### Code Obfuscation

It is a technique adopted by code developers to prevent reverse engineering on their code. Sometimes it obfuscates to alphabets but sometimes to weird punctuation like `ʼ/ˆⁱⁱᵢᵢᐧᐧ`.

| Catagory | obfuscated API | obfuscated package |
|----------|:--------------:|-------------------:|
| benign   |      1.27%     |             0.263% |
| malware  |     0.962%     |              0.55% |

It can be observed that benign apps have a higher rate of obfuscated APIs to protect their code. But malware tend to have a much higher chance of using obfuscated custom package.<sup>2</sup> Thus, I think a higher ratio of obfuscated package can make an app more suspicious.

<span>[2]</span> <sup>The way I define "obfuscated" is very loose, intentionally. A package/API is defined as "obfuscated" only if there are absolutely no English letters in it. If a package is defined as "obfuscated", it can be more suspicious because it's usually a custom package that's being intentionally hidden.</sup>

### Code Reliability Assessment

There are a few limitations of my data:
 
- Data from APKpure.com cannot guranteed to be benign apps. I have followed a twitter that post up-to-date malware apps and I was able to find some of them on APKpure.com

- I didn't use all files because there are many problems when scaling up. So I do not have full coverage.

- The way I limitted the app size to be below 100MB can be biased toward smaller apps. This is not a big issue, though.
___

## Baseline Model

For my baseline model, I extracted the statistics from the code. As I've learned from the EDA process that there's a large discrepancy of unique API ratio between benign and malware apps, I included that feautre. Particularly, I have `num_api`, `num_unique_api`, `unique_api_ratio`,` num_package`, `num_unique_pacakge`, `unique_package_ratio`. I think these features can capture the difference between benign and malware apps well because EDA process already implied it. 

### Train data
`train_df.head()` 

| app_name             	| num_api 	| num_unique_api 	| unique_api_ratio 	| num_package 	| num_unique_pacakge 	| unique_package_ratio 	| label   	|
|----------------------	|---------	|----------------	|------------------	|-------------	|--------------------	|----------------------	|---------	|
| malware_VikingHorde  	| 12055   	| 5056           	| 0.419411         	| 12055       	| 913                	| 0.0757362            	| malware 	|
| malware_GingerMaster 	| 27751   	| 4324           	| 0.155814         	| 27751       	| 1124               	| 0.040503             	| malware 	|
| malware_FakeAngry    	| 36991   	| 4041           	| 0.109243         	| 36991       	| 1129               	| 0.0305209            	| malware 	|
| malware_Gorpo        	| 26274   	| 4202           	| 0.15993          	| 26274       	| 1198               	| 0.0455964            	| malware 	|
| malware_Youmi        	| 17488   	| 4223           	| 0.24148          	| 17488       	| 1142               	| 0.0653019            	| malware 	|

### Performance

| Model                	| Test size 	| Accuracy 	| F1 score 	| Recall 	| Precision 	|
|----------------------	|-----------	|----------	|----------	|--------	|-----------	|
| LogisticRegression 	| 27        	| 0.8929   	| 0.769    	| 0.625  	| 1.0       	|

Based on this prediction, the accuracy is pretty good but recall is just 0.625, which means the classifier needs to predict more "malware". However, since the test size is small, the number itself is not super informative. Rather, it proves that benigh and malware apps are classifiable. Since the test size is quite small, I'd rather not do any fine tune because it will merely overfit the test set.

---
Description of the Hindroid Approach to the problem, including an explanation of the graphs that make up the HIN and why they are appropriate for approaching the problem at hand

## Hindroid Paper
Hindroid's approach to this problem is analyzing the relationships among API calls. The researchers represent various API call semantics in heterogeous graphs.

Below is the four basic matrices/graphs that will be used for more complex metapaths, possiblely multi-kernels.

| G | Shape | Definition                                                               | Explanation                                                 |
|---|-------|--------------------------------------------------------------------------|-------------------------------------------------------------|
| A | `K x N` | aij = 1 if appi contains APIj,<br>0 otherwise                            | A represents the "have" relationship between APIs and apps  |
| B | `N x N` | bij = 1 if APIi and APIj co-exist in the same code block,<br>0 otherwise | B connects two APIs if they occur in same method code block |
| P | `N x N` | Pij = 1 if APIi and APIj use the same package,<br>0 otherwise            | P connects two APIs if they use the same package            |
| I | `N x N` | Iij = 1 if APIi and APIj use the same invoke method,<br>0 otherwise      | I connects two APIs if they use the same invoke method      |

For example, `ABA^T` represents that two APIs coexist in the same code block. `APBP^TA^T` represents two APIs in the same package coexist in the same code block. 

Since the starting matrix must be `A` and the end matrix must be `A^T`, the output must be `KxK`, where K is the number of apps. Then, what's left in the middle is known as kernel trick. It can capture complex semantics of API patterns, which will be learned by machine learning models and be used to classify benign and malware apps. Malware apps tend to excute some command in common such as getting root access and write in root, running some processes in background. The metapath in Hindroid can identitfy these patterns and count the number of occurances.

In fact, the train data are still quite different from common supervised learning tasks because we only have very few number of rows, 100ish and 1000ish, depending on number of apps. And the feature space is also the same of data size so it will have explosion of dimentionality problem.<sup>3</sup> Then, support vector machine kicks in because it tries to maximum the margin of two data in a higher space, rather than approximating some functions like neural networks or many other machine learning models do. 

Thus, SVM is a good pick for the dataset. Models like tree-based models, or parametric models, probably won't generalize well beause of its dimentionality problem.


<span>[3]</span> <sup> Because you can always find a linear combination of features that perfectly seperate the labels but it might not generalize.(correct me if I am wrong..)
</sup>

---

## Result


| Kernel 	| Accuracy 	| F1 score 	| Recall 	| Precision 	|
|:--------	|:----------	|:----------	|:--------	|:-----------	|
| AA^T   	| 0.933    	| 0.923    	| 0.857  	| 1.0       	|
| ABA^T  	| 0.75       | 0.5      	| 1.0      	| 0.3       |
| APA^T  	| 0.75     	| 0.666    	| 1.0    	| 0.5       	|
| APBP^TA^T 	| 0.75     	| 0.666    	| 1.0    	| 0.5       	|


`AA^T` kernel performed really well compared to the others. This is because for other kernels, I can only use partial data.<sup>4</sup> Based on this performace, it's not comparable to Hindroid's result. I am curious in knowing the technical details of how they managed to convert large amount of data into graphs. But, in my result, since all recalls except `AA^T` are 1, this means my classifier tends to classify more apps as malware. This still has some mer

<span>[4]</span> <sup> I wrote a generic function that it can construct any kernel from a DataFrame. So I had to use unstack() method. However, Pandas(version==0.23) unstack method has a limit of int32 and this disallowed me from incoperating more data. This problem was not raised in my previous test runs. The limitation of int32 was raised to an open issue at https://github.com/pandas-dev/pandas/issues/26314. I tried to downgrade Pandas to 0.21 to use it, but it's quite tricky to do so because it is related to many other packages. I just want to be honest in this report.
</sup>

## Conclusion

Based on my result, I am confident to say that Hindroid's approach should work. Although, we cannot eliminate the possibility that classifiers learned the discrepancy(mishandling) from data generation process. To me, the biggest challenge of this project is how to properly handle the complexity when constructing the graph. Even for Pandas, it's not designed for dealing with really large data. I'd really like to know how they managed to convert millions of API properly to graphs they want. 


