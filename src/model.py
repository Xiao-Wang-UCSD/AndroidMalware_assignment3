import numpy as np
import pandas as pd
import re
import os
import json
import networkx as nx
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
from tqdm import tqdm
from scipy import sparse
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import precision_score, \
        recall_score, confusion_matrix, classification_report, \
        accuracy_score, f1_score

# extract class information from the first three lines
def extract_class_info(data):
    try:
        dict = {}
        try:
            source = re.findall(r'^.class (.+);', data[0])[0]
        except:
            print('cannot extract file name',data[0])
            return None
        try:
            class_type = re.findall(r'^.class ([\w+\s]+)\sL', data[0])
        except:
            print('cannot extract class type in {}'.format(source))
            return None
        try:
            super_class = re.findall(r'^.super L(.+);', data[1])[0]
        except:
            print('cannot extract super class in {}'.format(source))
            return None
        if class_type:
            dict['class_type'] = class_type[0]
        else:
            dict['class_type'] = None
        dict['super_class'] = super_class
        dict['class_name'] = source+'.smali'
    except:
        print('Empty file')
    return dict

def extract_class(file):
    try:
        data = file.readlines()
    except:
        return ['None']*5
    method_list = []
    api_list = []
    outside = True
    current_method = None
    count=0
    class_info = extract_class_info(data)
    if not class_info:
        return api_list
    
    for line in data:
        if not current_method:
            if check_method(line):
                current_method=check_method(line)
        elif check_exit(line):
            current_method = None
        else:
            if check_internal(line):
                api_list.append((check_internal(line)+(class_info['class_name']+'/'+current_method,)))
    return api_list

EXIT_RE = r'^.end method'
exit_re = re.compile(EXIT_RE)

def check_exit(line):
    method_end = body = exit_re.match(line)
    if method_end:
        return True
    return False

BODY_RE = r'^    invoke-([\w]+) .+, L(.+);->(.+)\(.*\)(.*).'
body_re = re.compile(BODY_RE)
def check_internal(line):
    body = body_re.match(line)
    if body:
        api_type = body[1]
        api_from = body[2]
        api_name = body[3]
        api_return = body[4]
        return api_type,api_from,api_name,api_return
    return None

METHOD_BODY_RE = r'^.method [\w+ ]*(\w+)\('
method_re = re.compile(METHOD_BODY_RE)
def check_method(line):
    method_body = method_re.match(line)
    if method_body:
        return method_body[1]
    return None
def check_reach_total_limit(count=0,limit=0):
    if count>=limit:
        print('file limit {} is reached'.format(file_limit))
        return True
    return False

def loop_all(path,file_limit,each_app_file_limit):
    count = 0
    api_df = pd.DataFrame()
    total_file = len(list(os.walk(path)))
    root_files = [x for x in os.listdir(path)]
    num_dict ={}
    for root, dirs, files in tqdm(os.walk(path),total=total_file):
        for file in files:
            if root!=path and file[-6:]=='.smali':
                app_name = root[len(path)+1:].split('/')[0]
                if not app_name in num_dict.keys():
                    num_dict[app_name] = 1
                elif num_dict[app_name]>=each_app_file_limit:
                    continue
                else:
                    num_dict[app_name]+=1 
                with open(os.path.join(root, file), "r") as auto:
                    with open(auto.name,'r') as file:
                        result = extract_class(file)
                        temp = pd.DataFrame(result,columns = ['invoke method','package name','API name','return type','method name'])
                        temp['app'] = path.split('/')[-1]+'_'+app_name
                        api_df = api_df.append(temp,ignore_index=True)
                        count+=1
                        if check_reach_total_limit(count,file_limit):
                            return api_df
    return api_df


# Catagory EDA
def catagory_EDA(api_df,catagory):
    temp = api_df[api_df['app'].apply(lambda x: catagory in x)]

    temp['API name'] = temp['package name']+" "+temp['API name']
    avg_num_api = len(temp['API name'])/temp['app'].nunique()
    avg_num_unique_api = temp['API name'].nunique()/temp['app'].nunique()
    unique_api_ratio = avg_num_unique_api/avg_num_api

    avg_num_package = len(temp['package name'])/temp['app'].nunique()
    avg_num_unique_package = temp['package name'].nunique()/temp['app'].nunique()
    unique_package_ratio = avg_num_unique_package/avg_num_package

    top5_api = temp['API name'].value_counts()[:5].index.to_list()
    top5_package = temp['package name'].value_counts()[:5].index.to_list()
    invoke_dist = temp['invoke method'].value_counts(normalize=True).round(3)
    
    G = get_matrix_A(temp,'app',return_graph=True)[2]
    graph_stats_dict = graph_stats(G)
    number_of_nodes = nx.number_of_nodes(G)
    number_of_edges = nx.number_of_edges(G)
    avg_degree = np.mean(list(dict(nx.degree(G)).values()))
    
    feature_result = {}
    feature_result['catagory'] = catagory
    feature_result['avg_num_api'] = avg_num_api
    feature_result['avg_num_unique_api'] = avg_num_unique_api
    feature_result['unique_api_ratio'] = unique_api_ratio
    feature_result['avg_num_package'] = avg_num_package
    feature_result['avg_num_unique_package'] = avg_num_unique_package
    feature_result['unique_package_ratio'] = unique_package_ratio
    feature_result['top5_api'] = top5_api
    feature_result['top5_package'] = top5_package
    feature_result['invoke_dist'] = invoke_dist
    feature_result['number_of_nodes'] = number_of_nodes
    feature_result['number_of_edges'] = number_of_edges
    feature_result['avg_degree'] = avg_degree
    
    return pd.DataFrame.from_dict(feature_result,orient='index').T  

# feature extraction for each app
def baseline_feature_extraction(df,app):
    temp = df[df['app']==app].copy()
    temp['API name'] = temp['package name']+" "+temp['API name']
    num_api = len(temp['API name'])
    num_unique_api = temp['API name'].nunique()
    unique_api_ratio = num_unique_api/num_api
    G = get_matrix_A(temp,'method name',return_graph=True)[2]
    graph_stats_dict = graph_stats(G)
    density = graph_stats_dict['density']
    corr = graph_stats_dict['corr']
    number_of_nodes = nx.number_of_nodes(G)
    number_of_edges = nx.number_of_edges(G)
    avg_degree = np.mean(list(dict(nx.degree(G)).values()))
    feature_result = {}
    feature_result['app_name'] = app
    feature_result['num_api'] = num_api
    feature_result['num_unique_api'] = num_unique_api
    feature_result['unique_api_ratio'] = unique_api_ratio
    feature_result['number_of_nodes'] = number_of_nodes
    feature_result['number_of_edges'] = number_of_edges
    feature_result['avg_degree'] = avg_degree
    feature_result['corr'] = corr
    feature_result['density'] = density
    feature_result['label'] = app.split('_')[0]
    return pd.DataFrame.from_dict(feature_result,orient='index').T


def filter_df(df):
    original_len = len(df)
    df = df[df['package name'].str.extract(r'^([A-Za-z])').notna().values]
    df = df[df['package name']!='java/lang/StringBuilder']
    filtered_len = len(df)
    # To prevent floating error
    print('Filtered',str(100*round(1-filtered_len/original_len,4))[:5]+'% rows')
    return df

def get_matrix(df,column,return_df=True):
    df = df.copy()
    df['count'] = 1
    df['API name'] =  df['package name']+df['API name']
    df = pd.crosstab(index=df['API name'], columns=df[column], values=df['count'], aggfunc='sum')
    matrix_A = df.fillna(0).to_numpy()
    matrix_A[matrix_A > 0] = 1
    if not return_df:
        return matrix_A.T
    return pd.DataFrame(data = matrix_A,columns = df.columns, index = df.index).T

def matrix_kernal(A,A_T,step_function=True):
    A = sparse.csr_matrix(A)
    A_T = sparse.csr_matrix(A_T)
    output = A @ A_T
    output[output > 0] = 1
    return output

def get_api_api_graph(df,matrix):
    api_api_df = pd.DataFrame(data = matrix,columns = df.index, index = df.index)
    df = pd.DataFrame(api_api_df.unstack()).reset_index(level = 1)
    df.columns = ['API name1','value']
    df = df.reset_index()
    df = df[(df['value']>=1)&(df['API name']!=df['API name1'])]
    return nx.from_pandas_edgelist(df, 'API name', 'API name1', edge_attr=True)

def graph_stats(G):
    density = nx.density(G)
    try:
        corr = nx.degree_pearson_correlation_coefficient(G)
    except:
        corr = 0
    avg_neighbor_degree = nx.average_neighbor_degree(G) #dict
    k_nearest_neighbors = nx.k_nearest_neighbors(G) #dict
    degree_centrality = nx.degree_centrality(G) #dict
    info = nx.info(G)
    return {'density':density,'corr':corr,'avg_neighbor_degree':avg_neighbor_degree,
           'k_nearest_neighbors':k_nearest_neighbors,'degree_centrality':degree_centrality,'info':info}

def multiply_matrix(*args):
    if len(args) == 1:
        return sparse.csr_matrix(args[0])
    else:
        return sparse.csr_matrix(args[0])@multiply_matrix(*args[1:])
def construct_kernal(*args,api_df,return_df=True):
    print('Computing kernal: {}'.format(''.join(args)))
    all_matrices = []
    app_name = []
    for i in range(len(args)):
        matrix_symbol = args[i]
        if 'A' in matrix_symbol:
            app_name = get_matrix(api_df,'app',return_df=True).index
            matrix_A = get_matrix(api_df,'app',return_df=False)
            if 'T' in matrix_symbol:
                all_matrices.append(matrix_A.T)
            else:
                all_matrices.append(matrix_A)
        elif 'B' in matrix_symbol:
            temp = get_matrix(api_df,'method name',return_df=False)
            matrix_B = matrix_kernal(temp.T,temp)
            if 'T' in matrix_symbol:
                all_matrices.append(matrix_B.T)
            else:
                all_matrices.append(matrix_B)
        elif 'P' in matrix_symbol:
            temp = get_matrix(api_df,'package name',return_df=False)
            matrix_P = matrix_kernal(temp.T,temp)
            if 'T' in matrix_symbol:
                all_matrices.append(matrix_P.T)
            else:
                all_matrices.append(matrix_P)
        elif 'I' in matrix_symbol:
            temp = get_matrix(api_df,'invoke method',return_df=False)
            matrix_I = matrix_kernal(temp.T,temp)
            if 'T' in matrix_symbol:
                all_matrices.append(matrix_I.T)
            else:
                all_matrices.append(matrix_I)
        else:
            print('Only support A,B,P,I matrices')
    if return_df:
        return pd.DataFrame(multiply_matrix(*all_matrices).todense(),index = app_name,columns = app_name)
    return multiply_matrix(*all_matrices)


def train_SVM(df):
    df = df.copy().sample(frac=1)
    df['label'] = df.index.to_series().apply(lambda x: x.split('_')[0])
    X,y = df.drop('label',axis=1),df['label']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
    clf = SVC(gamma='auto')
    clf.fit(X_train, y_train)
    TN, FP, FN, TP = confusion_matrix(y_test, clf.predict(X_test)).ravel()
    prediction = clf.predict(X_test)
    print('Accuracy:', accuracy_score(y_test, prediction))
    print('F1 score:', f1_score(y_test, prediction,pos_label = 'benign'))
    print('Recall:', recall_score(y_test, prediction,pos_label = 'benign'))
    print('Precision:', precision_score(y_test, prediction,pos_label = 'benign'))
    
def run(each_app_file_limit,total_file_limit,smali_path):
    if 'testing' in smali_path:
        print('Testing mode begins')
        print('Extracting benign apps')
        benigh_df = loop_all(smali_path+'/benign',total_file_limit,each_app_file_limit)
        print('Extracting malware apps \nShut down immediately if you are concerned :)')
        malware_df = loop_all(smali_path+'/malware',total_file_limit,each_app_file_limit)
        combined_df = benigh_df.append(malware_df)
        combined_df = filter_df(combined_df)
        result_kernal_df = construct_kernal('A','P','A_T',api_df = combined_df,return_df=True)
        if not os.path.exists('./result'):
            os.makedirs('./result')
        result_kernal_df.to_csv('./result/result.csv',index=False)
        train_SVM(result_kernal_df)
        return result_kernal_df
    df = loop_all(smali_path,total_file_limit,each_app_file_limit)
    df = filter_df(df)
    return df