import requests
from bs4 import BeautifulSoup
import io
import gzip
import random
import os.path
import re
from tqdm import tqdm
import json
from collections import defaultdict
import subprocess


# This crawler will only work for apkpure.com
base_url = 'https://apkpure.com'
VERBOSE = False

def get_all_sitemap_links(url,catagory = []):
    soup = BeautifulSoup(requests.get(url).text, 'xml')
    all_url = (i.find('loc').text for i in soup.find_all('sitemap'))
    if catagory:
        select_url = defaultdict(list)
        for i in catagory:
            for url in (i.find('loc').text for i in soup.find_all('sitemap')):
                if i in url:
                    select_url[i].append(url)
        return select_url
    return all_url

def find_gzip_link(url):
    with gzip.GzipFile(fileobj=io.BytesIO(requests.get(url).content)) as f:
        soup = BeautifulSoup(f.read(), 'xml')
        return (x.get('href') for x in soup.find_all("xhtml:link", {
            "hreflang": "x-default"
        })[5:] if not x.get('media') and len(x.get('href').split('/')) == 5)

def download_from_app_page(url,size_limit):
    soup = BeautifulSoup(requests.get(url).text, 'html.parser')
    url = soup.find('a',{'class':'da'})
    if url is not None:
        try:
            r = requests.get(base_url + url.get('href'))
            soup = BeautifulSoup(r.text, 'html.parser')
            link = soup.find('a', {'id': 'download_link'})
            fsize_tag = soup.find('span', {'class': 'fsize'})
            fsize = float(re.findall(r'[\d+|\.]+', fsize_tag.text)[0])
            app_name = url.get('href').split('/')[1]
        except:
            return None
        
        if link and fsize < size_limit:
            if VERBOSE:
                print('Will download "{}"'.format(app_name))
            return app_name,link.get('href')
        else:
            if link:
                return None
                if VERBOSE:
                    print('Unable to download "{}" \nSize {} mb exceedes limit {} mb'.format(app_name,fsize,size_limit))
            return None

def download_apks(download_amount,size_limit,catagory,apk_path,convert_to_smali):
    to_download = []
    print('Looking for {} apps in {}'.format(download_amount,', '.join(catagory)))
    dic = get_all_sitemap_links(base_url + '/sitemap.xml',catagory)
    for x in dic:
        each_catagory_count = 0
        for i in dic[x]:
            if each_catagory_count>=download_amount:
                continue
            for link in find_gzip_link(i):
                if each_catagory_count>=download_amount:
                    continue
                temp = download_from_app_page(link,size_limit)
                if temp and temp[0] not in [j[0] for j in to_download] and download_amount>0:
                    each_catagory_count+=1
                    to_download.append((temp,x))
    print('Begin downloading',int(download_amount)*len(catagory),'apps...')
    for name_url,x in tqdm(to_download):
        r = requests.get(name_url[1])
        # If the name is too long, then it's a werid app. I will not download it
        if len(name_url[0])>50:
            print('file name too long for app: ',name_url[0][:10])
            print("If the name is too long, then it's a werid app. I will not download it")
            continue
        with open(apk_path+"/"+x+'_'+name_url[0] + '.apk', 'wb') as f:
            f.write(r.content)
            print('"{}" downloaded'.format(name_url[0]))
            pass
        if VERBOSE:
            print('"{}" downloaded'.format(name_url[0]))
    print('Finsh downloading {} apps in {}'.format(download_amount,', '.join(catagory)))
    print('Begin decompiling to smali...')
    if convert_to_smali:
        convert_to_smali()
    return

def convert_to_smali():
    cmd = subprocess.check_output(["./temp.sh",apk_path.split('/')[-1]])
    print(str(cmd, "utf-8"))
    return cmd

def test():
    print('GOOD!!')
    return